{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kera_first_network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_J39VpywM1n",
        "colab_type": "text"
      },
      "source": [
        "# **CLASSIFICATION TASK ON THE  PIMA INDIANS DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tci9LAkuuwB5",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will do a simple classification task on the  Pima Indians onset of diabetes dataset which is a classic dataset that can be found on the UCI Machine Learning repository (https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv,https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names). This dataset describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years (1) or not (0).This is a binary classification problem that we'll try to resolve here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEYxVkinq1sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We import the librairies\n",
        "import os\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFd4FjrAuNtu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "002ccfc2-cb47-4ddd-d0fe-18eef2053259"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GfREWnhwZq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6fac5e20-41ab-4444-eaab-e7923299dede"
      },
      "source": [
        "#Let's explore the dataset file for a moment to see what's inside\n",
        "myfile='/content/drive/My Drive/Colab Notebooks/DL/pima_indians_diabetes.csv'\n",
        "df=pd.read_csv(myfile,names=['Number of times pregnant',' Plasma glucose concentration','Diastolic Blood Pressure','Triceps skin fold thickness','2-Hour serum insulin','Body mass index ','Diabetes pedigree function','Age(years)','Class variable'])\n",
        "df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number of times pregnant</th>\n",
              "      <th>Plasma glucose concentration</th>\n",
              "      <th>Diastolic Blood Pressure</th>\n",
              "      <th>Triceps skin fold thickness</th>\n",
              "      <th>2-Hour serum insulin</th>\n",
              "      <th>Body mass index</th>\n",
              "      <th>Diabetes pedigree function</th>\n",
              "      <th>Age(years)</th>\n",
              "      <th>Class variable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Number of times pregnant  ...  Class variable\n",
              "0                         6  ...               1\n",
              "1                         1  ...               0\n",
              "2                         8  ...               1\n",
              "3                         1  ...               0\n",
              "4                         0  ...               1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwh2vyQ7zEdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now, let's load the data and separate the inputs(X) from the labels(Y--> the last column of the file)\n",
        "data=loadtxt(myfile,delimiter=',')\n",
        "X=data[:,0:8]\n",
        "Y=data[:,8]\n",
        "#Let's split the dataset into train set and test set\n",
        "Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,test_size=0.1,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0O-b7Zp0qtu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1101e10-0a8a-4560-9baf-cf9ff935f546"
      },
      "source": [
        "#Now, let's define our model structure\n",
        "model=Sequential()\n",
        "model.add(Dense(12,activation='relu',input_dim=8))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "#Let's compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "#Let's fit the model\n",
        "model.fit(Xtrain,Ytrain,batch_size=10,epochs=300,validation_data=(Xtest,Ytest),verbose=1)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 691 samples, validate on 77 samples\n",
            "Epoch 1/300\n",
            "691/691 [==============================] - 0s 484us/step - loss: 2.9455 - acc: 0.5543 - val_loss: 1.8297 - val_acc: 0.6494\n",
            "Epoch 2/300\n",
            "691/691 [==============================] - 0s 134us/step - loss: 1.7078 - acc: 0.5340 - val_loss: 1.3763 - val_acc: 0.6623\n",
            "Epoch 3/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 1.2169 - acc: 0.5369 - val_loss: 0.9921 - val_acc: 0.6494\n",
            "Epoch 4/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.9891 - acc: 0.5745 - val_loss: 0.9022 - val_acc: 0.5325\n",
            "Epoch 5/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.9210 - acc: 0.6035 - val_loss: 0.7543 - val_acc: 0.6104\n",
            "Epoch 6/300\n",
            "691/691 [==============================] - 0s 154us/step - loss: 0.8166 - acc: 0.6223 - val_loss: 0.7489 - val_acc: 0.5974\n",
            "Epoch 7/300\n",
            "691/691 [==============================] - 0s 141us/step - loss: 0.7599 - acc: 0.6440 - val_loss: 0.7363 - val_acc: 0.6104\n",
            "Epoch 8/300\n",
            "691/691 [==============================] - 0s 115us/step - loss: 0.7341 - acc: 0.6570 - val_loss: 0.7458 - val_acc: 0.6234\n",
            "Epoch 9/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.7051 - acc: 0.6729 - val_loss: 0.6965 - val_acc: 0.6623\n",
            "Epoch 10/300\n",
            "691/691 [==============================] - 0s 141us/step - loss: 0.7058 - acc: 0.6700 - val_loss: 0.6832 - val_acc: 0.6753\n",
            "Epoch 11/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.6898 - acc: 0.6614 - val_loss: 0.6815 - val_acc: 0.6494\n",
            "Epoch 12/300\n",
            "691/691 [==============================] - 0s 150us/step - loss: 0.6757 - acc: 0.6860 - val_loss: 0.6788 - val_acc: 0.6623\n",
            "Epoch 13/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.6684 - acc: 0.6773 - val_loss: 0.6574 - val_acc: 0.6883\n",
            "Epoch 14/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.6721 - acc: 0.6657 - val_loss: 0.6566 - val_acc: 0.6623\n",
            "Epoch 15/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.6635 - acc: 0.6729 - val_loss: 0.6904 - val_acc: 0.6494\n",
            "Epoch 16/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.6417 - acc: 0.6918 - val_loss: 0.6524 - val_acc: 0.6364\n",
            "Epoch 17/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.6381 - acc: 0.6990 - val_loss: 0.6799 - val_acc: 0.6494\n",
            "Epoch 18/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.6199 - acc: 0.6961 - val_loss: 0.6318 - val_acc: 0.6753\n",
            "Epoch 19/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.6177 - acc: 0.6903 - val_loss: 0.6729 - val_acc: 0.6494\n",
            "Epoch 20/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.6004 - acc: 0.7135 - val_loss: 0.6576 - val_acc: 0.6494\n",
            "Epoch 21/300\n",
            "691/691 [==============================] - 0s 149us/step - loss: 0.6062 - acc: 0.6932 - val_loss: 0.6630 - val_acc: 0.6623\n",
            "Epoch 22/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.5997 - acc: 0.6990 - val_loss: 0.6490 - val_acc: 0.6623\n",
            "Epoch 23/300\n",
            "691/691 [==============================] - 0s 155us/step - loss: 0.6036 - acc: 0.6990 - val_loss: 0.5900 - val_acc: 0.6883\n",
            "Epoch 24/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.5905 - acc: 0.7048 - val_loss: 0.6199 - val_acc: 0.6753\n",
            "Epoch 25/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.5865 - acc: 0.6961 - val_loss: 0.5733 - val_acc: 0.6883\n",
            "Epoch 26/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.5975 - acc: 0.7091 - val_loss: 0.5864 - val_acc: 0.6883\n",
            "Epoch 27/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.5820 - acc: 0.7164 - val_loss: 0.5938 - val_acc: 0.6753\n",
            "Epoch 28/300\n",
            "691/691 [==============================] - 0s 155us/step - loss: 0.5876 - acc: 0.7004 - val_loss: 0.5637 - val_acc: 0.7013\n",
            "Epoch 29/300\n",
            "691/691 [==============================] - 0s 147us/step - loss: 0.5789 - acc: 0.7106 - val_loss: 0.6290 - val_acc: 0.6494\n",
            "Epoch 30/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.5846 - acc: 0.6975 - val_loss: 0.5687 - val_acc: 0.6883\n",
            "Epoch 31/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5805 - acc: 0.7019 - val_loss: 0.6009 - val_acc: 0.6623\n",
            "Epoch 32/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5715 - acc: 0.7135 - val_loss: 0.6046 - val_acc: 0.6623\n",
            "Epoch 33/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5681 - acc: 0.7164 - val_loss: 0.7252 - val_acc: 0.5714\n",
            "Epoch 34/300\n",
            "691/691 [==============================] - 0s 136us/step - loss: 0.5712 - acc: 0.7004 - val_loss: 0.6312 - val_acc: 0.6364\n",
            "Epoch 35/300\n",
            "691/691 [==============================] - 0s 136us/step - loss: 0.5698 - acc: 0.7178 - val_loss: 0.6624 - val_acc: 0.6234\n",
            "Epoch 36/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.5770 - acc: 0.6975 - val_loss: 0.5577 - val_acc: 0.6753\n",
            "Epoch 37/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5691 - acc: 0.7135 - val_loss: 0.5725 - val_acc: 0.7143\n",
            "Epoch 38/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.5614 - acc: 0.7149 - val_loss: 0.5745 - val_acc: 0.6753\n",
            "Epoch 39/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5718 - acc: 0.7135 - val_loss: 0.6780 - val_acc: 0.6234\n",
            "Epoch 40/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.5700 - acc: 0.7135 - val_loss: 0.6885 - val_acc: 0.6364\n",
            "Epoch 41/300\n",
            "691/691 [==============================] - 0s 147us/step - loss: 0.5854 - acc: 0.6990 - val_loss: 0.5833 - val_acc: 0.6623\n",
            "Epoch 42/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.5575 - acc: 0.7062 - val_loss: 0.5635 - val_acc: 0.7013\n",
            "Epoch 43/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.5640 - acc: 0.7149 - val_loss: 0.6995 - val_acc: 0.6104\n",
            "Epoch 44/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5523 - acc: 0.7265 - val_loss: 0.5905 - val_acc: 0.6753\n",
            "Epoch 45/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5615 - acc: 0.7221 - val_loss: 0.6269 - val_acc: 0.6234\n",
            "Epoch 46/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.5576 - acc: 0.7279 - val_loss: 0.5976 - val_acc: 0.6623\n",
            "Epoch 47/300\n",
            "691/691 [==============================] - 0s 150us/step - loss: 0.5686 - acc: 0.7135 - val_loss: 0.5774 - val_acc: 0.6623\n",
            "Epoch 48/300\n",
            "691/691 [==============================] - 0s 134us/step - loss: 0.5588 - acc: 0.7236 - val_loss: 0.5912 - val_acc: 0.7013\n",
            "Epoch 49/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5535 - acc: 0.7192 - val_loss: 0.5784 - val_acc: 0.6494\n",
            "Epoch 50/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.5522 - acc: 0.7250 - val_loss: 0.5802 - val_acc: 0.6623\n",
            "Epoch 51/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.5460 - acc: 0.7207 - val_loss: 0.5798 - val_acc: 0.6494\n",
            "Epoch 52/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.5564 - acc: 0.7178 - val_loss: 0.5597 - val_acc: 0.7143\n",
            "Epoch 53/300\n",
            "691/691 [==============================] - 0s 149us/step - loss: 0.5482 - acc: 0.7323 - val_loss: 0.5627 - val_acc: 0.6364\n",
            "Epoch 54/300\n",
            "691/691 [==============================] - 0s 143us/step - loss: 0.5476 - acc: 0.7294 - val_loss: 0.6227 - val_acc: 0.6494\n",
            "Epoch 55/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.5549 - acc: 0.7294 - val_loss: 0.5867 - val_acc: 0.6234\n",
            "Epoch 56/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.5556 - acc: 0.7178 - val_loss: 0.5514 - val_acc: 0.6753\n",
            "Epoch 57/300\n",
            "691/691 [==============================] - 0s 167us/step - loss: 0.5411 - acc: 0.7221 - val_loss: 0.5425 - val_acc: 0.7143\n",
            "Epoch 58/300\n",
            "691/691 [==============================] - 0s 140us/step - loss: 0.5386 - acc: 0.7395 - val_loss: 0.5775 - val_acc: 0.6364\n",
            "Epoch 59/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.5471 - acc: 0.7149 - val_loss: 0.5855 - val_acc: 0.6364\n",
            "Epoch 60/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.5380 - acc: 0.7294 - val_loss: 0.5946 - val_acc: 0.5974\n",
            "Epoch 61/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.5464 - acc: 0.7366 - val_loss: 0.5644 - val_acc: 0.6364\n",
            "Epoch 62/300\n",
            "691/691 [==============================] - 0s 121us/step - loss: 0.5565 - acc: 0.7250 - val_loss: 0.6863 - val_acc: 0.5974\n",
            "Epoch 63/300\n",
            "691/691 [==============================] - 0s 147us/step - loss: 0.5476 - acc: 0.7221 - val_loss: 0.6004 - val_acc: 0.6104\n",
            "Epoch 64/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.5430 - acc: 0.7236 - val_loss: 0.6206 - val_acc: 0.5974\n",
            "Epoch 65/300\n",
            "691/691 [==============================] - 0s 140us/step - loss: 0.5379 - acc: 0.7308 - val_loss: 0.6908 - val_acc: 0.5974\n",
            "Epoch 66/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.5615 - acc: 0.7149 - val_loss: 0.6749 - val_acc: 0.6234\n",
            "Epoch 67/300\n",
            "691/691 [==============================] - 0s 143us/step - loss: 0.5313 - acc: 0.7467 - val_loss: 0.5566 - val_acc: 0.7273\n",
            "Epoch 68/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5316 - acc: 0.7467 - val_loss: 0.5523 - val_acc: 0.7013\n",
            "Epoch 69/300\n",
            "691/691 [==============================] - 0s 152us/step - loss: 0.5359 - acc: 0.7366 - val_loss: 0.5773 - val_acc: 0.6494\n",
            "Epoch 70/300\n",
            "691/691 [==============================] - 0s 150us/step - loss: 0.5222 - acc: 0.7410 - val_loss: 0.6003 - val_acc: 0.6234\n",
            "Epoch 71/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.5403 - acc: 0.7337 - val_loss: 0.6139 - val_acc: 0.6234\n",
            "Epoch 72/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.5357 - acc: 0.7381 - val_loss: 0.5498 - val_acc: 0.7273\n",
            "Epoch 73/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.5309 - acc: 0.7554 - val_loss: 0.6185 - val_acc: 0.6234\n",
            "Epoch 74/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.5384 - acc: 0.7178 - val_loss: 0.6272 - val_acc: 0.5974\n",
            "Epoch 75/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5597 - acc: 0.7149 - val_loss: 0.6453 - val_acc: 0.6234\n",
            "Epoch 76/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5332 - acc: 0.7467 - val_loss: 0.6873 - val_acc: 0.6234\n",
            "Epoch 77/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.5304 - acc: 0.7467 - val_loss: 0.6576 - val_acc: 0.6234\n",
            "Epoch 78/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.5298 - acc: 0.7438 - val_loss: 0.6000 - val_acc: 0.7273\n",
            "Epoch 79/300\n",
            "691/691 [==============================] - 0s 148us/step - loss: 0.5347 - acc: 0.7511 - val_loss: 0.6119 - val_acc: 0.6234\n",
            "Epoch 80/300\n",
            "691/691 [==============================] - 0s 142us/step - loss: 0.5472 - acc: 0.7337 - val_loss: 0.5917 - val_acc: 0.6234\n",
            "Epoch 81/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.5301 - acc: 0.7511 - val_loss: 0.5796 - val_acc: 0.6623\n",
            "Epoch 82/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5372 - acc: 0.7381 - val_loss: 0.5792 - val_acc: 0.6753\n",
            "Epoch 83/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5228 - acc: 0.7395 - val_loss: 0.6013 - val_acc: 0.6234\n",
            "Epoch 84/300\n",
            "691/691 [==============================] - 0s 121us/step - loss: 0.5287 - acc: 0.7482 - val_loss: 0.5738 - val_acc: 0.6364\n",
            "Epoch 85/300\n",
            "691/691 [==============================] - 0s 139us/step - loss: 0.5199 - acc: 0.7583 - val_loss: 0.6750 - val_acc: 0.6364\n",
            "Epoch 86/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.5266 - acc: 0.7381 - val_loss: 0.6306 - val_acc: 0.6364\n",
            "Epoch 87/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.5217 - acc: 0.7482 - val_loss: 0.5767 - val_acc: 0.6883\n",
            "Epoch 88/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5251 - acc: 0.7424 - val_loss: 0.6569 - val_acc: 0.6104\n",
            "Epoch 89/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5163 - acc: 0.7424 - val_loss: 0.6743 - val_acc: 0.6623\n",
            "Epoch 90/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.5175 - acc: 0.7496 - val_loss: 0.5910 - val_acc: 0.6234\n",
            "Epoch 91/300\n",
            "691/691 [==============================] - 0s 155us/step - loss: 0.5278 - acc: 0.7540 - val_loss: 0.5625 - val_acc: 0.7013\n",
            "Epoch 92/300\n",
            "691/691 [==============================] - 0s 152us/step - loss: 0.5204 - acc: 0.7395 - val_loss: 0.5713 - val_acc: 0.6623\n",
            "Epoch 93/300\n",
            "691/691 [==============================] - 0s 139us/step - loss: 0.5161 - acc: 0.7482 - val_loss: 0.6264 - val_acc: 0.6494\n",
            "Epoch 94/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.5258 - acc: 0.7453 - val_loss: 0.5934 - val_acc: 0.6364\n",
            "Epoch 95/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.5153 - acc: 0.7540 - val_loss: 0.5734 - val_acc: 0.6623\n",
            "Epoch 96/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5186 - acc: 0.7540 - val_loss: 0.5807 - val_acc: 0.6494\n",
            "Epoch 97/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5158 - acc: 0.7511 - val_loss: 0.6258 - val_acc: 0.6623\n",
            "Epoch 98/300\n",
            "691/691 [==============================] - 0s 120us/step - loss: 0.5177 - acc: 0.7511 - val_loss: 0.5912 - val_acc: 0.7143\n",
            "Epoch 99/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.5267 - acc: 0.7381 - val_loss: 0.7038 - val_acc: 0.6234\n",
            "Epoch 100/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.5252 - acc: 0.7410 - val_loss: 0.6149 - val_acc: 0.6494\n",
            "Epoch 101/300\n",
            "691/691 [==============================] - 0s 138us/step - loss: 0.5137 - acc: 0.7554 - val_loss: 0.6108 - val_acc: 0.6623\n",
            "Epoch 102/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.5137 - acc: 0.7496 - val_loss: 0.6375 - val_acc: 0.6623\n",
            "Epoch 103/300\n",
            "691/691 [==============================] - 0s 137us/step - loss: 0.5201 - acc: 0.7569 - val_loss: 0.6914 - val_acc: 0.6494\n",
            "Epoch 104/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.5335 - acc: 0.7308 - val_loss: 0.7931 - val_acc: 0.5714\n",
            "Epoch 105/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.5239 - acc: 0.7381 - val_loss: 0.5506 - val_acc: 0.7143\n",
            "Epoch 106/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5159 - acc: 0.7598 - val_loss: 0.5575 - val_acc: 0.6883\n",
            "Epoch 107/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.5106 - acc: 0.7612 - val_loss: 0.5897 - val_acc: 0.6623\n",
            "Epoch 108/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.5152 - acc: 0.7554 - val_loss: 0.5598 - val_acc: 0.6883\n",
            "Epoch 109/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5103 - acc: 0.7569 - val_loss: 0.5912 - val_acc: 0.6494\n",
            "Epoch 110/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.5038 - acc: 0.7641 - val_loss: 0.5546 - val_acc: 0.7273\n",
            "Epoch 111/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.5115 - acc: 0.7583 - val_loss: 0.5819 - val_acc: 0.6494\n",
            "Epoch 112/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5163 - acc: 0.7525 - val_loss: 0.5949 - val_acc: 0.6753\n",
            "Epoch 113/300\n",
            "691/691 [==============================] - 0s 140us/step - loss: 0.5299 - acc: 0.7381 - val_loss: 0.5874 - val_acc: 0.6753\n",
            "Epoch 114/300\n",
            "691/691 [==============================] - 0s 136us/step - loss: 0.5155 - acc: 0.7554 - val_loss: 0.6019 - val_acc: 0.6234\n",
            "Epoch 115/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.5001 - acc: 0.7598 - val_loss: 0.5575 - val_acc: 0.6623\n",
            "Epoch 116/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.4987 - acc: 0.7583 - val_loss: 0.6204 - val_acc: 0.6494\n",
            "Epoch 117/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5051 - acc: 0.7612 - val_loss: 0.5698 - val_acc: 0.6623\n",
            "Epoch 118/300\n",
            "691/691 [==============================] - 0s 137us/step - loss: 0.5040 - acc: 0.7656 - val_loss: 0.6132 - val_acc: 0.6364\n",
            "Epoch 119/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.5016 - acc: 0.7670 - val_loss: 0.5330 - val_acc: 0.7143\n",
            "Epoch 120/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5159 - acc: 0.7395 - val_loss: 0.5790 - val_acc: 0.6623\n",
            "Epoch 121/300\n",
            "691/691 [==============================] - 0s 136us/step - loss: 0.5025 - acc: 0.7583 - val_loss: 0.5456 - val_acc: 0.7792\n",
            "Epoch 122/300\n",
            "691/691 [==============================] - 0s 138us/step - loss: 0.5068 - acc: 0.7569 - val_loss: 0.5262 - val_acc: 0.7532\n",
            "Epoch 123/300\n",
            "691/691 [==============================] - 0s 137us/step - loss: 0.5115 - acc: 0.7656 - val_loss: 0.5428 - val_acc: 0.7143\n",
            "Epoch 124/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.5142 - acc: 0.7395 - val_loss: 0.5650 - val_acc: 0.6753\n",
            "Epoch 125/300\n",
            "691/691 [==============================] - 0s 137us/step - loss: 0.5078 - acc: 0.7525 - val_loss: 0.5203 - val_acc: 0.7792\n",
            "Epoch 126/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.5033 - acc: 0.7525 - val_loss: 0.7417 - val_acc: 0.6494\n",
            "Epoch 127/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5020 - acc: 0.7598 - val_loss: 0.6076 - val_acc: 0.6883\n",
            "Epoch 128/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5120 - acc: 0.7554 - val_loss: 0.6391 - val_acc: 0.6364\n",
            "Epoch 129/300\n",
            "691/691 [==============================] - 0s 120us/step - loss: 0.5007 - acc: 0.7612 - val_loss: 0.5716 - val_acc: 0.6753\n",
            "Epoch 130/300\n",
            "691/691 [==============================] - 0s 147us/step - loss: 0.5020 - acc: 0.7482 - val_loss: 0.6099 - val_acc: 0.6623\n",
            "Epoch 131/300\n",
            "691/691 [==============================] - 0s 136us/step - loss: 0.4971 - acc: 0.7627 - val_loss: 0.5686 - val_acc: 0.6753\n",
            "Epoch 132/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.5040 - acc: 0.7627 - val_loss: 0.5376 - val_acc: 0.7532\n",
            "Epoch 133/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.4971 - acc: 0.7728 - val_loss: 0.5361 - val_acc: 0.7143\n",
            "Epoch 134/300\n",
            "691/691 [==============================] - 0s 143us/step - loss: 0.4967 - acc: 0.7742 - val_loss: 0.5712 - val_acc: 0.6753\n",
            "Epoch 135/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4919 - acc: 0.7656 - val_loss: 0.5398 - val_acc: 0.7403\n",
            "Epoch 136/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.5011 - acc: 0.7511 - val_loss: 0.5583 - val_acc: 0.6883\n",
            "Epoch 137/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.5161 - acc: 0.7525 - val_loss: 0.5626 - val_acc: 0.7013\n",
            "Epoch 138/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4975 - acc: 0.7583 - val_loss: 0.5758 - val_acc: 0.6753\n",
            "Epoch 139/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.5040 - acc: 0.7641 - val_loss: 0.5895 - val_acc: 0.6883\n",
            "Epoch 140/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.4863 - acc: 0.7685 - val_loss: 0.6178 - val_acc: 0.6883\n",
            "Epoch 141/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.4978 - acc: 0.7699 - val_loss: 0.5995 - val_acc: 0.7013\n",
            "Epoch 142/300\n",
            "691/691 [==============================] - 0s 136us/step - loss: 0.5066 - acc: 0.7641 - val_loss: 0.5652 - val_acc: 0.7013\n",
            "Epoch 143/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4971 - acc: 0.7656 - val_loss: 0.6224 - val_acc: 0.7013\n",
            "Epoch 144/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4885 - acc: 0.7786 - val_loss: 0.6222 - val_acc: 0.6234\n",
            "Epoch 145/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4929 - acc: 0.7598 - val_loss: 0.5711 - val_acc: 0.7273\n",
            "Epoch 146/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.5029 - acc: 0.7554 - val_loss: 0.5913 - val_acc: 0.7013\n",
            "Epoch 147/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.5106 - acc: 0.7410 - val_loss: 0.5579 - val_acc: 0.6753\n",
            "Epoch 148/300\n",
            "691/691 [==============================] - 0s 148us/step - loss: 0.4915 - acc: 0.7656 - val_loss: 0.5236 - val_acc: 0.7403\n",
            "Epoch 149/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4953 - acc: 0.7554 - val_loss: 0.5663 - val_acc: 0.7273\n",
            "Epoch 150/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4979 - acc: 0.7525 - val_loss: 0.5688 - val_acc: 0.6883\n",
            "Epoch 151/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4868 - acc: 0.7612 - val_loss: 0.5146 - val_acc: 0.7403\n",
            "Epoch 152/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4955 - acc: 0.7699 - val_loss: 0.5544 - val_acc: 0.7143\n",
            "Epoch 153/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.4957 - acc: 0.7612 - val_loss: 0.5813 - val_acc: 0.6623\n",
            "Epoch 154/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.4923 - acc: 0.7670 - val_loss: 0.5976 - val_acc: 0.6753\n",
            "Epoch 155/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.5110 - acc: 0.7496 - val_loss: 0.5191 - val_acc: 0.7532\n",
            "Epoch 156/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4892 - acc: 0.7627 - val_loss: 0.5744 - val_acc: 0.7143\n",
            "Epoch 157/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.4888 - acc: 0.7670 - val_loss: 0.5734 - val_acc: 0.7013\n",
            "Epoch 158/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.4899 - acc: 0.7612 - val_loss: 0.5648 - val_acc: 0.7143\n",
            "Epoch 159/300\n",
            "691/691 [==============================] - 0s 146us/step - loss: 0.4773 - acc: 0.7757 - val_loss: 0.6389 - val_acc: 0.6753\n",
            "Epoch 160/300\n",
            "691/691 [==============================] - 0s 138us/step - loss: 0.5046 - acc: 0.7453 - val_loss: 0.5395 - val_acc: 0.7532\n",
            "Epoch 161/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.4871 - acc: 0.7656 - val_loss: 0.5685 - val_acc: 0.7143\n",
            "Epoch 162/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.4830 - acc: 0.7670 - val_loss: 0.5240 - val_acc: 0.7273\n",
            "Epoch 163/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4918 - acc: 0.7815 - val_loss: 0.5519 - val_acc: 0.7532\n",
            "Epoch 164/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4851 - acc: 0.7771 - val_loss: 0.5421 - val_acc: 0.7532\n",
            "Epoch 165/300\n",
            "691/691 [==============================] - 0s 150us/step - loss: 0.4941 - acc: 0.7873 - val_loss: 0.5643 - val_acc: 0.6753\n",
            "Epoch 166/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.4941 - acc: 0.7699 - val_loss: 0.5454 - val_acc: 0.7143\n",
            "Epoch 167/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.4900 - acc: 0.7554 - val_loss: 0.5112 - val_acc: 0.7532\n",
            "Epoch 168/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.4903 - acc: 0.7583 - val_loss: 0.5256 - val_acc: 0.7662\n",
            "Epoch 169/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4851 - acc: 0.7728 - val_loss: 0.5369 - val_acc: 0.7273\n",
            "Epoch 170/300\n",
            "691/691 [==============================] - 0s 141us/step - loss: 0.4796 - acc: 0.7742 - val_loss: 0.6611 - val_acc: 0.7013\n",
            "Epoch 171/300\n",
            "691/691 [==============================] - 0s 140us/step - loss: 0.4853 - acc: 0.7713 - val_loss: 0.5905 - val_acc: 0.6883\n",
            "Epoch 172/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4890 - acc: 0.7612 - val_loss: 0.5178 - val_acc: 0.7403\n",
            "Epoch 173/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4848 - acc: 0.7728 - val_loss: 0.5344 - val_acc: 0.7273\n",
            "Epoch 174/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.4823 - acc: 0.7713 - val_loss: 0.5612 - val_acc: 0.7143\n",
            "Epoch 175/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4754 - acc: 0.7829 - val_loss: 0.5660 - val_acc: 0.7013\n",
            "Epoch 176/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4919 - acc: 0.7627 - val_loss: 0.5772 - val_acc: 0.7013\n",
            "Epoch 177/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4770 - acc: 0.7685 - val_loss: 0.5355 - val_acc: 0.7273\n",
            "Epoch 178/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4872 - acc: 0.7583 - val_loss: 0.5454 - val_acc: 0.7143\n",
            "Epoch 179/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.4841 - acc: 0.7670 - val_loss: 0.4895 - val_acc: 0.7922\n",
            "Epoch 180/300\n",
            "691/691 [==============================] - 0s 134us/step - loss: 0.4847 - acc: 0.7699 - val_loss: 0.6007 - val_acc: 0.6883\n",
            "Epoch 181/300\n",
            "691/691 [==============================] - 0s 140us/step - loss: 0.4777 - acc: 0.7786 - val_loss: 0.5329 - val_acc: 0.7143\n",
            "Epoch 182/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.4956 - acc: 0.7656 - val_loss: 0.5269 - val_acc: 0.7013\n",
            "Epoch 183/300\n",
            "691/691 [==============================] - 0s 147us/step - loss: 0.4858 - acc: 0.7670 - val_loss: 0.5565 - val_acc: 0.7013\n",
            "Epoch 184/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4907 - acc: 0.7496 - val_loss: 0.5282 - val_acc: 0.7532\n",
            "Epoch 185/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4825 - acc: 0.7771 - val_loss: 0.5881 - val_acc: 0.6753\n",
            "Epoch 186/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4824 - acc: 0.7598 - val_loss: 0.5895 - val_acc: 0.6883\n",
            "Epoch 187/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.4941 - acc: 0.7525 - val_loss: 0.6058 - val_acc: 0.7013\n",
            "Epoch 188/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4974 - acc: 0.7685 - val_loss: 0.5681 - val_acc: 0.7143\n",
            "Epoch 189/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4863 - acc: 0.7685 - val_loss: 0.5618 - val_acc: 0.7143\n",
            "Epoch 190/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4723 - acc: 0.7641 - val_loss: 0.5609 - val_acc: 0.7273\n",
            "Epoch 191/300\n",
            "691/691 [==============================] - 0s 119us/step - loss: 0.4713 - acc: 0.7757 - val_loss: 0.6144 - val_acc: 0.6623\n",
            "Epoch 192/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4822 - acc: 0.7641 - val_loss: 0.5367 - val_acc: 0.7532\n",
            "Epoch 193/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4737 - acc: 0.7713 - val_loss: 0.5288 - val_acc: 0.7532\n",
            "Epoch 194/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.4749 - acc: 0.7554 - val_loss: 0.5572 - val_acc: 0.7013\n",
            "Epoch 195/300\n",
            "691/691 [==============================] - 0s 155us/step - loss: 0.4835 - acc: 0.7670 - val_loss: 0.5833 - val_acc: 0.7013\n",
            "Epoch 196/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.4898 - acc: 0.7728 - val_loss: 0.5429 - val_acc: 0.7403\n",
            "Epoch 197/300\n",
            "691/691 [==============================] - 0s 156us/step - loss: 0.4760 - acc: 0.7742 - val_loss: 0.5649 - val_acc: 0.7403\n",
            "Epoch 198/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4776 - acc: 0.7670 - val_loss: 0.5761 - val_acc: 0.7403\n",
            "Epoch 199/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.4743 - acc: 0.7670 - val_loss: 0.5611 - val_acc: 0.7013\n",
            "Epoch 200/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.4903 - acc: 0.7598 - val_loss: 0.5751 - val_acc: 0.7273\n",
            "Epoch 201/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4710 - acc: 0.7656 - val_loss: 0.5545 - val_acc: 0.7273\n",
            "Epoch 202/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4689 - acc: 0.7656 - val_loss: 0.6270 - val_acc: 0.7013\n",
            "Epoch 203/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4971 - acc: 0.7554 - val_loss: 0.5974 - val_acc: 0.6883\n",
            "Epoch 204/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4804 - acc: 0.7699 - val_loss: 0.5273 - val_acc: 0.7403\n",
            "Epoch 205/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.4754 - acc: 0.7713 - val_loss: 0.5669 - val_acc: 0.7273\n",
            "Epoch 206/300\n",
            "691/691 [==============================] - 0s 140us/step - loss: 0.4746 - acc: 0.7699 - val_loss: 0.5502 - val_acc: 0.7532\n",
            "Epoch 207/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4690 - acc: 0.7873 - val_loss: 0.5230 - val_acc: 0.7792\n",
            "Epoch 208/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4713 - acc: 0.7974 - val_loss: 0.5602 - val_acc: 0.7273\n",
            "Epoch 209/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4754 - acc: 0.7786 - val_loss: 0.5382 - val_acc: 0.7532\n",
            "Epoch 210/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4707 - acc: 0.7656 - val_loss: 0.5204 - val_acc: 0.7403\n",
            "Epoch 211/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4695 - acc: 0.7771 - val_loss: 0.5668 - val_acc: 0.7403\n",
            "Epoch 212/300\n",
            "691/691 [==============================] - 0s 120us/step - loss: 0.4760 - acc: 0.7757 - val_loss: 0.4958 - val_acc: 0.7922\n",
            "Epoch 213/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4668 - acc: 0.7685 - val_loss: 0.6314 - val_acc: 0.6883\n",
            "Epoch 214/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.5277 - acc: 0.7525 - val_loss: 0.5004 - val_acc: 0.7143\n",
            "Epoch 215/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4889 - acc: 0.7540 - val_loss: 0.5450 - val_acc: 0.7143\n",
            "Epoch 216/300\n",
            "691/691 [==============================] - 0s 134us/step - loss: 0.4803 - acc: 0.7656 - val_loss: 0.5124 - val_acc: 0.7143\n",
            "Epoch 217/300\n",
            "691/691 [==============================] - 0s 154us/step - loss: 0.4829 - acc: 0.7583 - val_loss: 0.5168 - val_acc: 0.7532\n",
            "Epoch 218/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.4776 - acc: 0.7627 - val_loss: 0.5672 - val_acc: 0.7273\n",
            "Epoch 219/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4726 - acc: 0.7771 - val_loss: 0.5362 - val_acc: 0.7403\n",
            "Epoch 220/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4737 - acc: 0.7612 - val_loss: 0.5023 - val_acc: 0.7662\n",
            "Epoch 221/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4709 - acc: 0.7728 - val_loss: 0.5605 - val_acc: 0.7403\n",
            "Epoch 222/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4726 - acc: 0.7627 - val_loss: 0.5276 - val_acc: 0.7532\n",
            "Epoch 223/300\n",
            "691/691 [==============================] - 0s 133us/step - loss: 0.4817 - acc: 0.7771 - val_loss: 0.6069 - val_acc: 0.7143\n",
            "Epoch 224/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4711 - acc: 0.7771 - val_loss: 0.6146 - val_acc: 0.6883\n",
            "Epoch 225/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4809 - acc: 0.7583 - val_loss: 0.6096 - val_acc: 0.6623\n",
            "Epoch 226/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4825 - acc: 0.7627 - val_loss: 0.5843 - val_acc: 0.7403\n",
            "Epoch 227/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4645 - acc: 0.7844 - val_loss: 0.5379 - val_acc: 0.7403\n",
            "Epoch 228/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4764 - acc: 0.7713 - val_loss: 0.5654 - val_acc: 0.7403\n",
            "Epoch 229/300\n",
            "691/691 [==============================] - 0s 138us/step - loss: 0.4671 - acc: 0.7728 - val_loss: 0.5712 - val_acc: 0.7273\n",
            "Epoch 230/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.4765 - acc: 0.7815 - val_loss: 0.5608 - val_acc: 0.7273\n",
            "Epoch 231/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4699 - acc: 0.7800 - val_loss: 0.6805 - val_acc: 0.6753\n",
            "Epoch 232/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.4766 - acc: 0.7656 - val_loss: 0.6060 - val_acc: 0.7143\n",
            "Epoch 233/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4833 - acc: 0.7641 - val_loss: 0.6027 - val_acc: 0.7013\n",
            "Epoch 234/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4802 - acc: 0.7656 - val_loss: 0.5279 - val_acc: 0.7273\n",
            "Epoch 235/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4782 - acc: 0.7771 - val_loss: 0.5509 - val_acc: 0.7143\n",
            "Epoch 236/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4678 - acc: 0.7815 - val_loss: 0.5013 - val_acc: 0.7792\n",
            "Epoch 237/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4762 - acc: 0.7757 - val_loss: 0.5340 - val_acc: 0.7662\n",
            "Epoch 238/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4650 - acc: 0.7685 - val_loss: 0.5274 - val_acc: 0.7273\n",
            "Epoch 239/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4726 - acc: 0.7742 - val_loss: 0.6009 - val_acc: 0.7013\n",
            "Epoch 240/300\n",
            "691/691 [==============================] - 0s 142us/step - loss: 0.4680 - acc: 0.7771 - val_loss: 0.5444 - val_acc: 0.7273\n",
            "Epoch 241/300\n",
            "691/691 [==============================] - 0s 139us/step - loss: 0.4686 - acc: 0.7742 - val_loss: 0.5166 - val_acc: 0.7273\n",
            "Epoch 242/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4614 - acc: 0.7829 - val_loss: 0.6215 - val_acc: 0.6623\n",
            "Epoch 243/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4647 - acc: 0.7858 - val_loss: 0.5859 - val_acc: 0.7273\n",
            "Epoch 244/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4823 - acc: 0.7583 - val_loss: 0.5596 - val_acc: 0.7273\n",
            "Epoch 245/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4601 - acc: 0.7902 - val_loss: 0.5773 - val_acc: 0.7403\n",
            "Epoch 246/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4690 - acc: 0.7728 - val_loss: 0.5532 - val_acc: 0.7273\n",
            "Epoch 247/300\n",
            "691/691 [==============================] - 0s 120us/step - loss: 0.4629 - acc: 0.7742 - val_loss: 0.6056 - val_acc: 0.7143\n",
            "Epoch 248/300\n",
            "691/691 [==============================] - 0s 123us/step - loss: 0.4657 - acc: 0.7699 - val_loss: 0.5843 - val_acc: 0.7403\n",
            "Epoch 249/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4706 - acc: 0.7800 - val_loss: 0.5252 - val_acc: 0.7792\n",
            "Epoch 250/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4718 - acc: 0.7699 - val_loss: 0.5887 - val_acc: 0.7273\n",
            "Epoch 251/300\n",
            "691/691 [==============================] - 0s 153us/step - loss: 0.4655 - acc: 0.7815 - val_loss: 0.5964 - val_acc: 0.7143\n",
            "Epoch 252/300\n",
            "691/691 [==============================] - 0s 139us/step - loss: 0.4709 - acc: 0.7685 - val_loss: 0.5339 - val_acc: 0.7273\n",
            "Epoch 253/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.4571 - acc: 0.7829 - val_loss: 0.5620 - val_acc: 0.7532\n",
            "Epoch 254/300\n",
            "691/691 [==============================] - 0s 137us/step - loss: 0.4657 - acc: 0.7699 - val_loss: 0.5505 - val_acc: 0.7403\n",
            "Epoch 255/300\n",
            "691/691 [==============================] - 0s 121us/step - loss: 0.4828 - acc: 0.7713 - val_loss: 0.5444 - val_acc: 0.7273\n",
            "Epoch 256/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4690 - acc: 0.7757 - val_loss: 0.5067 - val_acc: 0.7273\n",
            "Epoch 257/300\n",
            "691/691 [==============================] - 0s 148us/step - loss: 0.4625 - acc: 0.7786 - val_loss: 0.5157 - val_acc: 0.7273\n",
            "Epoch 258/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4641 - acc: 0.7685 - val_loss: 0.5754 - val_acc: 0.7143\n",
            "Epoch 259/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4633 - acc: 0.7829 - val_loss: 0.5656 - val_acc: 0.7662\n",
            "Epoch 260/300\n",
            "691/691 [==============================] - 0s 121us/step - loss: 0.4621 - acc: 0.7815 - val_loss: 0.5287 - val_acc: 0.7662\n",
            "Epoch 261/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4573 - acc: 0.7757 - val_loss: 0.5201 - val_acc: 0.7403\n",
            "Epoch 262/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4624 - acc: 0.7858 - val_loss: 0.5534 - val_acc: 0.7273\n",
            "Epoch 263/300\n",
            "691/691 [==============================] - 0s 144us/step - loss: 0.4744 - acc: 0.7829 - val_loss: 0.5342 - val_acc: 0.7532\n",
            "Epoch 264/300\n",
            "691/691 [==============================] - 0s 143us/step - loss: 0.4615 - acc: 0.7931 - val_loss: 0.5068 - val_acc: 0.7532\n",
            "Epoch 265/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4665 - acc: 0.7670 - val_loss: 0.5458 - val_acc: 0.6883\n",
            "Epoch 266/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4585 - acc: 0.7858 - val_loss: 0.6852 - val_acc: 0.6753\n",
            "Epoch 267/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4702 - acc: 0.7569 - val_loss: 0.5680 - val_acc: 0.7403\n",
            "Epoch 268/300\n",
            "691/691 [==============================] - 0s 131us/step - loss: 0.4584 - acc: 0.7931 - val_loss: 0.5497 - val_acc: 0.7532\n",
            "Epoch 269/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4752 - acc: 0.7786 - val_loss: 0.5949 - val_acc: 0.7403\n",
            "Epoch 270/300\n",
            "691/691 [==============================] - 0s 122us/step - loss: 0.4660 - acc: 0.7873 - val_loss: 0.6717 - val_acc: 0.6753\n",
            "Epoch 271/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4658 - acc: 0.7829 - val_loss: 0.5742 - val_acc: 0.7403\n",
            "Epoch 272/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4653 - acc: 0.7815 - val_loss: 0.6044 - val_acc: 0.7273\n",
            "Epoch 273/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.4574 - acc: 0.7916 - val_loss: 0.5328 - val_acc: 0.7662\n",
            "Epoch 274/300\n",
            "691/691 [==============================] - 0s 135us/step - loss: 0.4855 - acc: 0.7742 - val_loss: 0.5319 - val_acc: 0.7403\n",
            "Epoch 275/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.4551 - acc: 0.7713 - val_loss: 0.5747 - val_acc: 0.7532\n",
            "Epoch 276/300\n",
            "691/691 [==============================] - 0s 130us/step - loss: 0.4571 - acc: 0.7786 - val_loss: 0.5234 - val_acc: 0.7403\n",
            "Epoch 277/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4539 - acc: 0.7786 - val_loss: 0.5732 - val_acc: 0.7143\n",
            "Epoch 278/300\n",
            "691/691 [==============================] - 0s 121us/step - loss: 0.4584 - acc: 0.7873 - val_loss: 0.5971 - val_acc: 0.7143\n",
            "Epoch 279/300\n",
            "691/691 [==============================] - 0s 124us/step - loss: 0.4760 - acc: 0.7670 - val_loss: 0.5815 - val_acc: 0.7143\n",
            "Epoch 280/300\n",
            "691/691 [==============================] - 0s 127us/step - loss: 0.4653 - acc: 0.7757 - val_loss: 0.6004 - val_acc: 0.7403\n",
            "Epoch 281/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4500 - acc: 0.7815 - val_loss: 0.5346 - val_acc: 0.7662\n",
            "Epoch 282/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4628 - acc: 0.7858 - val_loss: 0.5446 - val_acc: 0.7532\n",
            "Epoch 283/300\n",
            "691/691 [==============================] - 0s 138us/step - loss: 0.4491 - acc: 0.7873 - val_loss: 0.5751 - val_acc: 0.7532\n",
            "Epoch 284/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.4518 - acc: 0.7887 - val_loss: 0.5737 - val_acc: 0.7273\n",
            "Epoch 285/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.4688 - acc: 0.7916 - val_loss: 0.5914 - val_acc: 0.7403\n",
            "Epoch 286/300\n",
            "691/691 [==============================] - 0s 136us/step - loss: 0.4594 - acc: 0.7887 - val_loss: 0.6221 - val_acc: 0.7013\n",
            "Epoch 287/300\n",
            "691/691 [==============================] - 0s 145us/step - loss: 0.4648 - acc: 0.7887 - val_loss: 0.5798 - val_acc: 0.7273\n",
            "Epoch 288/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4514 - acc: 0.7829 - val_loss: 0.6170 - val_acc: 0.7013\n",
            "Epoch 289/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4641 - acc: 0.7916 - val_loss: 0.5808 - val_acc: 0.7013\n",
            "Epoch 290/300\n",
            "691/691 [==============================] - 0s 121us/step - loss: 0.4537 - acc: 0.7959 - val_loss: 0.7094 - val_acc: 0.7013\n",
            "Epoch 291/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4749 - acc: 0.7685 - val_loss: 0.5834 - val_acc: 0.7532\n",
            "Epoch 292/300\n",
            "691/691 [==============================] - 0s 121us/step - loss: 0.4568 - acc: 0.7713 - val_loss: 0.5965 - val_acc: 0.7273\n",
            "Epoch 293/300\n",
            "691/691 [==============================] - 0s 126us/step - loss: 0.4559 - acc: 0.7829 - val_loss: 0.5471 - val_acc: 0.7532\n",
            "Epoch 294/300\n",
            "691/691 [==============================] - 0s 153us/step - loss: 0.4602 - acc: 0.7902 - val_loss: 0.5585 - val_acc: 0.7662\n",
            "Epoch 295/300\n",
            "691/691 [==============================] - 0s 159us/step - loss: 0.4664 - acc: 0.7757 - val_loss: 0.5639 - val_acc: 0.7532\n",
            "Epoch 296/300\n",
            "691/691 [==============================] - 0s 132us/step - loss: 0.4563 - acc: 0.7945 - val_loss: 0.5527 - val_acc: 0.7662\n",
            "Epoch 297/300\n",
            "691/691 [==============================] - 0s 153us/step - loss: 0.4577 - acc: 0.7786 - val_loss: 0.6175 - val_acc: 0.7013\n",
            "Epoch 298/300\n",
            "691/691 [==============================] - 0s 129us/step - loss: 0.4591 - acc: 0.7713 - val_loss: 0.5532 - val_acc: 0.7532\n",
            "Epoch 299/300\n",
            "691/691 [==============================] - 0s 128us/step - loss: 0.4568 - acc: 0.7829 - val_loss: 0.6626 - val_acc: 0.7273\n",
            "Epoch 300/300\n",
            "691/691 [==============================] - 0s 125us/step - loss: 0.4589 - acc: 0.7815 - val_loss: 0.5405 - val_acc: 0.7662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1eff6ebe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swclI-Ex86Ze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "168e3607-b7fe-4df9-cbbf-c9c6e441ebae"
      },
      "source": [
        "#Let's do predictions on the test set\n",
        "pred=model.predict_classes(Xtest)\n",
        "for i in range(10,20):\n",
        "\tprint('%s => %d (expected %d)' % (Xtest[i].tolist(), pred[i], Ytest[i]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6.0, 80.0, 80.0, 36.0, 0.0, 39.8, 0.177, 28.0] => 0 (expected 0)\n",
            "[2.0, 92.0, 62.0, 28.0, 0.0, 31.6, 0.13, 24.0] => 0 (expected 0)\n",
            "[4.0, 145.0, 82.0, 18.0, 0.0, 32.5, 0.235, 70.0] => 1 (expected 1)\n",
            "[2.0, 129.0, 0.0, 0.0, 0.0, 38.5, 0.304, 41.0] => 0 (expected 0)\n",
            "[0.0, 179.0, 90.0, 27.0, 0.0, 44.1, 0.686, 23.0] => 1 (expected 1)\n",
            "[0.0, 93.0, 100.0, 39.0, 72.0, 43.4, 1.021, 35.0] => 0 (expected 0)\n",
            "[11.0, 103.0, 68.0, 40.0, 0.0, 46.2, 0.126, 42.0] => 1 (expected 0)\n",
            "[2.0, 121.0, 70.0, 32.0, 95.0, 39.1, 0.886, 23.0] => 0 (expected 0)\n",
            "[2.0, 134.0, 70.0, 0.0, 0.0, 28.9, 0.542, 23.0] => 0 (expected 1)\n",
            "[1.0, 109.0, 56.0, 21.0, 135.0, 25.2, 0.833, 23.0] => 0 (expected 0)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}